{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk6NzDR3dMNl"
      },
      "source": [
        "# CA02: Spam Email Detection Using Naive Bayes Classification Algorithm\n",
        "\n",
        "<i>\n",
        "Luis Otero\n",
        "<br>\n",
        "BSAN 6070\n",
        "<br>\n",
        "January 30, 2024\n",
        "<br>\n",
        "https://github.com/otero106/BSAN6070\n",
        "</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHsNCamGdMNn"
      },
      "source": [
        "## Project Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi8h9eVydMNo"
      },
      "source": [
        "In this Python project, we will be utilizing a supervised machine learning algorithm called Naive Bayes to build a model that can determine whether an email is spam or not. The algorithm is based on Bayes' Theorem, which is a mathematical formula used to find the probability of an event occuring when we are given the probability of another event that has already occurred. Another important fundamental in Naive Bayes is the \"naive\" part, meaning we are assuming that each feature in a provided dataset is conditionally independent from one another. For this project, we will be importing two folders containing a mix of spam and non spam emails, one is for training our model and one is testing the accuracy of the model. Then we will train our model by implementing it on our training folder and this will make it learn to discern which email is spam or not based on the words within the emails. Subsequently, we will implement the same model on the testing folder to compute the accuracy and if it could actually categorize emails as spam or non-spam."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python Implementation"
      ],
      "metadata": {
        "id": "lQiJJtXR_rQ3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp2i396cdMNo"
      },
      "source": [
        "### Part 1: Loading Libraries and Accessing Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqGTWsbYdMNo"
      },
      "source": [
        "In order to run certain lines of code in this project, we first need to import certain libraries. We will be using the OS module to interact with the operating system of the computer that this program is running on. NumPy is a Python library for working with arrays and mathematical functions. Counter is a sub-class of the collections module, used to count hashable objects in Python. We will be using scikit-learn to import the Gaussian Naive-Bayes and metrics to assess model performance. Finally, we will import Google drive to access the folders and files located on the local user's account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "4p_DvtT7sOIr"
      },
      "outputs": [],
      "source": [
        "#importing libraries to pre-process emails\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "#import naive bayes and metrics from scikit-learn\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "\n",
        "#import google drive\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iqXQzsVD4kcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a54b17a-f6f9-4281-8cff-5f7f7300b7a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#accessing local files that are in google drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Writing Functions to Process Emails"
      ],
      "metadata": {
        "id": "62pWswZm_Pp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to use functions that will process the each and every email that is in the train-mails and test-mails folders. We will ignore the subject lines of the emails and concentrate on the actual email content and file name. In here, the functions called make_Dictionary and extract_features will remove non-required words, expressions and symbols from text. The function extract_features will produce a matrix representation of the word frequency."
      ],
      "metadata": {
        "id": "lm8Bh3zZXBRU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "jjKF0nIMwz8_"
      },
      "outputs": [],
      "source": [
        "#function to make a dictionary of each word in email and their frequency\n",
        "def make_Dictionary(root_dir):\n",
        "  all_words = [] #empty list to store all words\n",
        "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)] #list of all files in folder\n",
        "\n",
        "  #iterate through each and every email and turn every sentence into list of words\n",
        "  for mail in emails:\n",
        "    with open(mail) as m:\n",
        "      for line in m:\n",
        "        words = line.split()\n",
        "        all_words += words #add list of words to list all_words\n",
        "\n",
        "  #make a dictionary of all words and their count\n",
        "  dictionary = Counter(all_words)\n",
        "  list_to_remove = list(dictionary) #make list of all keys (words) in dictionary\n",
        "\n",
        "  #iterate through list and remove words unnecessary words\n",
        "  for item in list_to_remove:\n",
        "    if item.isalpha() == False:\n",
        "      del dictionary[item] #if word is not alphabet, remove from dictionary\n",
        "    elif len(item) == 1:\n",
        "      del dictionary[item] #if word is only one character, remove from dictionary\n",
        "\n",
        "  #turn dictionary into a list of the 3000 most common words and their count\n",
        "  dictionary = dictionary.most_common(3000)\n",
        "  return dictionary #output of function is dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "dmVW5xNlyOFc"
      },
      "outputs": [],
      "source": [
        "#function to\n",
        "def extract_features(mail_dir):\n",
        "\n",
        "  #make a list of all emails(files) in folder and make matrices of zeroes\n",
        "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
        "  features_matrix = np.zeros((len(files),3000)) #size of matrix is no. of files and 3000\n",
        "  train_labels = np.zeros(len(files)) #size of matrix is no. of files\n",
        "\n",
        "  #variables to keep track of spam emails and documents\n",
        "  count = 1;\n",
        "  docID = 0;\n",
        "\n",
        "  #iterate through every line in every email\n",
        "  for fil in files:\n",
        "    with open(fil) as fi:\n",
        "\n",
        "      #if line is third line, turn it into a list of words and iterate thru list\n",
        "      #skip subject line and go to email content\n",
        "      for i, line in enumerate(fi):\n",
        "        if i ==2:\n",
        "          words = line.split()\n",
        "          for word in words:\n",
        "            wordID = 0 #variable to keep track which word it is\n",
        "\n",
        "            #loop thru dictionary of words created from previous function and\n",
        "            #check if word is in dictionary\n",
        "            for i, d in enumerate(dictionary):\n",
        "              if d[0] == word:\n",
        "                wordID = i #if yes, set word ID to index\n",
        "                features_matrix[docID,wordID] = words.count(word) #set count of word in matrix\n",
        "\n",
        "      #set label of file and split file path to list\n",
        "      train_labels[docID] = 0;\n",
        "      filepathTokens = fil.split('/')\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1] #get last character in list\n",
        "\n",
        "      #check if email is spam if the file path starts with spmsg\n",
        "      if lastToken.startswith(\"spmsg\"):\n",
        "        train_labels[docID] = 1;\n",
        "        count = count + 1 #increase spam count by 1\n",
        "      docID = docID + 1 #increase document ID by 1\n",
        "\n",
        "  #output features matrix and labels\n",
        "  return features_matrix, train_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Load Training and Test Emails"
      ],
      "metadata": {
        "id": "hf94j0ks_m6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need data to model! We need to specify the paths of the folders that contain the data. In this case, my folders are located on my google drive so I will use their associated file paths. If you are running this notebook on your colab environment in your own account, please make the appropriate changes. Refer to the README that is located in the GitHub folder of this assignment."
      ],
      "metadata": {
        "id": "WbTjnNuwd1wA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "zoq-rE7Mx0pp"
      },
      "outputs": [],
      "source": [
        "#enter the file path of your \"train_mails\" and \"test-mails\" folders located on your Google Drive\n",
        "train_path = '/content/drive/MyDrive/Colab Notebooks/BSAN6070/Computer_Assignments/CA02/train-mails'\n",
        "test_path = '/content/drive/MyDrive/Colab Notebooks/BSAN6070/Computer_Assignments/CA02/test-mails'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4: Cleaning All Emails"
      ],
      "metadata": {
        "id": "0b2nCPQNV3Wh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now use our functions to process all the data in the train and test folders. With the outputs, we can jump right into using a Naive Bayes model."
      ],
      "metadata": {
        "id": "beNkOMJedBGe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134lmhauyQxE",
        "outputId": "f3e7f256-296c-43d0-c898-bef0abb8434a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n"
          ]
        }
      ],
      "source": [
        "#using the make_Dictionary function on training data, store all words from training data in dictionary\n",
        "dictionary = make_Dictionary(train_path)\n",
        "\n",
        "#make features matrix and labels on both training and test data\n",
        "print(\"reading and processing emails from TRAIN and TEST folders\")\n",
        "features_matrix, labels = extract_features(train_path)\n",
        "test_features_matrix, test_labels = extract_features(test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 5: Training and Testing the Model"
      ],
      "metadata": {
        "id": "SElU66Ht_n1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last step of this project is to create our Naive Bayes model using the scikit-learn package and train the model using the email training data we have cleaned for this purpose. We then make predictions on the prepared testing data and retrieve the accuracy of our model."
      ],
      "metadata": {
        "id": "bJFwEBp8a0zL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vupr3TF2dMNr",
        "outputId": "a6f09cca-86dd-43f2-9d25-cc7f8f61c2bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model using Gaussian Naive Bayes algorithm\n",
            "Training completed\n",
            "Testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels: 0.9615384615384616\n"
          ]
        }
      ],
      "source": [
        "#training the model using Naive Bayes algorithm\n",
        "print(\"Training Model using Gaussian Naive Bayes algorithm\")\n",
        "print(\"Training completed\")\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(features_matrix, labels)\n",
        "\n",
        "#making predictions on testing set\n",
        "print(\"Testing trained model to predict Test Data labels\")\n",
        "test_pred = gnb.predict(test_features_matrix)\n",
        "\n",
        "#comparing actual response values (test_labels) with predicted response values (test_pred)\n",
        "#testing accuracy of model\n",
        "print(\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\",\n",
        "      metrics.accuracy_score(test_labels, test_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}